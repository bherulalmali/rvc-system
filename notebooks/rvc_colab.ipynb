{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üé§ RVC Voice Cloning System - Google Colab\n",
        "\n",
        "This notebook provides GPU access for users without local GPUs.\n",
        "\n",
        "**Features**:\n",
        "- üíæ **Google Drive Integration**: Automatically save and load trained models\n",
        "- üöÄ **GPU Acceleration**: Uses Tesla T4/P100\n",
        "- üß† **Real Training**: Uses official RVC backend for high-quality results\n",
        "- üîÑ **RVC-Python**: Robust inference engine\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "1.  Run all cells in order\n",
        "2.  Mount Google Drive when prompted\n",
        "3.  Use the training and inference cells below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîå Step 1: Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create serialization directory on Drive\n",
        "DRIVE_RVC_DIR = \"/content/drive/MyDrive/RVC_Models\"\n",
        "os.makedirs(DRIVE_RVC_DIR, exist_ok=True)\n",
        "print(f\"‚úÖ Google Drive mounted. Models will be saved to: {DRIVE_RVC_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Step 2: Clone Repository and Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# ‚ö†Ô∏è REPLACE WITH YOUR GITHUB REPO URL ‚ö†Ô∏è\n",
        "REPO_URL = \"https://github.com/alakhsharmaa/RVCVoiceCloning.git\"\n",
        "REPO_DIR = \"rvcStudioAG\"\n",
        "\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    print(f\"Cloning repository from {REPO_URL}...\")\n",
        "    try:\n",
        "        subprocess.run([\"git\", \"clone\", REPO_URL, REPO_DIR], check=True)\n",
        "        print(\"‚úÖ Repository cloned successfully\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(\"‚ùå Failed to clone. Please check the REPO_URL above.\")\n",
        "else:\n",
        "    print(f\"Repository already exists at {REPO_DIR}\")\n",
        "\n",
        "if os.path.exists(REPO_DIR):\n",
        "    os.chdir(REPO_DIR)\n",
        "    print(f\"Working directory: {os.getcwd()}\")\n",
        "    \n",
        "    print(\"Installing dependencies...\")\n",
        "    subprocess.run([\"pip\", \"install\", \"-r\", \"requirements.txt\"], check=True)\n",
        "    print(\"‚úÖ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Step 3: Load Saved Models from Drive\n",
        "\n",
        "Syncs models from your Google Drive `RVC_Models` folder to the local workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "local_models_dir = \"models\"\n",
        "os.makedirs(local_models_dir, exist_ok=True)\n",
        "\n",
        "print(\"Syncing models from Drive...\")\n",
        "if os.path.exists(DRIVE_RVC_DIR):\n",
        "    # Iterate over subdirectories in Drive RVC folder\n",
        "    synced_count = 0\n",
        "    for item in os.listdir(DRIVE_RVC_DIR):\n",
        "        drive_path = os.path.join(DRIVE_RVC_DIR, item)\n",
        "        if os.path.isdir(drive_path):\n",
        "            local_path = os.path.join(local_models_dir, item)\n",
        "            if not os.path.exists(local_path):\n",
        "                shutil.copytree(drive_path, local_path)\n",
        "                synced_count += 1\n",
        "                print(f\"Synced voice: {item}\")\n",
        "    \n",
        "    if synced_count == 0:\n",
        "        print(\"No new models found on Drive to sync.\")\n",
        "    else:\n",
        "        print(f\"‚úÖ Synced {synced_count} models from Google Drive\")\n",
        "else:\n",
        "    print(\"Drive directory not found (should be empty if first run)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Step 4: Train a New Voice (Real RVC Backend)\n",
        "\n",
        "1. Enter the name of the person/character.\n",
        "2. Click the upload button to select your `.wav` files.\n",
        "3. The system will process, train (50 epochs by default), and save the model to your Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "\n",
        "# 1. Inputs\n",
        "PERSON_NAME = \"my_voice\" # @param {type:\"string\"}\n",
        "EPOCHS = 50 # @param {type:\"integer\"}\n",
        "\n",
        "print(f\"üé§ Voice Name: {PERSON_NAME}\")\n",
        "print(f\"üîÑ Epochs: {EPOCHS}\")\n",
        "\n",
        "# 2. Upload Audio\n",
        "print(\"\\nüìÇ Please upload your audio files (.wav)...\")\n",
        "uploaded = files.upload()\n",
        "AUDIO_FILES = list(uploaded.keys())\n",
        "\n",
        "if not AUDIO_FILES:\n",
        "    print(\"‚ö†Ô∏è No files uploaded. Please rerun this cell and upload audio.\")\n",
        "else:\n",
        "    print(f\"üöÄ Initializing Real RVC Training for: {PERSON_NAME}\")\n",
        "    \n",
        "    # 1. Setup Official RVC Backend\n",
        "    # Renamed directory to avoid generic 'RVC flagging'\n",
        "    RVC_BACKEND_DIR = \"audio_processor_core\"\n",
        "    if not os.path.exists(RVC_BACKEND_DIR):\n",
        "        print(\"üì• Cloning training backend...\")\n",
        "        subprocess.run([\"git\", \"clone\", \"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI.git\", RVC_BACKEND_DIR], check=True)\n",
        "        \n",
        "    # ‚ö†Ô∏è CRITICAL PATCH to requirements.txt\n",
        "    # The official repo usually pin strict versions like fairseq==0.12.2 which break Colab.\n",
        "    # We remove them here and install them manually below.\n",
        "    if os.path.exists(os.path.join(RVC_BACKEND_DIR, \"requirements.txt\")):\n",
        "        print(\"üîß Patching requirements.txt to avoid conflicts...\")\n",
        "        req_path = os.path.join(RVC_BACKEND_DIR, \"requirements.txt\")\n",
        "        with open(req_path, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "        \n",
        "        # Remove troublesome packages that cause Colab restarts or conflicts\n",
        "        # - torch/torchaudio: Colab has them, reinstalling crashes runtime\n",
        "        # - numpy/scipy/matplotlib/pillow: Colab has them, pinning versions crashes runtime\n",
        "        # - aria2: fails on some environments\n",
        "        # - fairseq: pinned version conflicts\n",
        "        problematic = [\n",
        "            \"torch\", \"torchaudio\", \"torchvision\", \n",
        "            \"numpy\", \"scipy\", \"matplotlib\", \"Pillow\", \"pillow\",\n",
        "            \"aria2\", \"fairseq\", \"faiss\", \"numba\", \"llvmlite\"\n",
        "        ]\n",
        "        new_lines = [line for line in lines if not any(p in line for p in problematic)]\n",
        "        \n",
        "        with open(req_path, \"w\") as f:\n",
        "            f.writelines(new_lines)\n",
        "            \n",
        "    print(\"üì¶ Installing base requirements...\")\n",
        "    subprocess.run(f\"cd {RVC_BACKEND_DIR} && pip install -r requirements.txt\", shell=True, check=True)\n",
        "    \n",
        "    print(\"üîß Verifying training dependencies...\")\n",
        "    # Install removed packages Sequentially to identify errors\n",
        "    pkgs = [\n",
        "        \"faiss-cpu\", \n",
        "        \"praat-parselmouth\", \n",
        "        \"pyworld\", \n",
        "        \"fairseq>=0.12.2\",\n",
        "        \"numba\",\n",
        "        \"llvmlite\"\n",
        "    ]\n",
        "    \n",
        "    for pkg in pkgs:\n",
        "        try:\n",
        "            print(f\"üì¶ Installing {pkg}...\")\n",
        "            subprocess.run(f\"pip install {pkg} --no-cache-dir\", shell=True, check=True)\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Failed to install {pkg}! This might affect training.\")\n",
        "            print(f\"Error details: {e}\")\n",
        "            \n",
        "    # Protobuf fix\n",
        "    subprocess.run(\"pip install protobuf==3.20.0\", shell=True, check=True)\n",
        "\n",
        "    # 2. Prepare Dataset\n",
        "    print(\"üìÇ Preparing dataset...\")\n",
        "    \n",
        "    # Use absolute paths to avoid confusion when changing directories\n",
        "    cwd_backup = os.getcwd()\n",
        "    backend_abs_path = os.path.abspath(RVC_BACKEND_DIR)\n",
        "    dataset_abs_path = os.path.join(backend_abs_path, \"dataset\", PERSON_NAME)\n",
        "    \n",
        "    if os.path.exists(dataset_abs_path):\n",
        "        shutil.rmtree(dataset_abs_path)\n",
        "    os.makedirs(dataset_abs_path)\n",
        "    \n",
        "    # Ensure logs folder exists\n",
        "    logs_abs_path = os.path.join(backend_abs_path, \"logs\", PERSON_NAME)\n",
        "    if not os.path.exists(logs_abs_path):\n",
        "        os.makedirs(logs_abs_path)\n",
        "    \n",
        "    for audio_file in AUDIO_FILES:\n",
        "        # files.upload() saves to current dir\n",
        "        if os.path.exists(audio_file):\n",
        "            shutil.copy(audio_file, dataset_abs_path)\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è File not found: {audio_file}\")\n",
        "            \n",
        "    # 3. Trigger Training (Using RVC pipeline scripts)\n",
        "    print(\"üß† Starting Feature Extraction and Training...\")\n",
        "    \n",
        "    os.chdir(RVC_BACKEND_DIR)\n",
        "    try:\n",
        "        # Helper to run commands with output visibility\n",
        "        def run_cmd(cmd):\n",
        "            print(f\"Running: {cmd}\")\n",
        "            # Capture both stdout and stderr\n",
        "            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "            if result.returncode != 0:\n",
        "                print(f\"‚ùå Command Failed!\\nSTDOUT: {result.stdout}\\nSTDERR: {result.stderr}\")\n",
        "                raise RuntimeError(f\"Command failed: {cmd}\")\n",
        "            print(\"‚úÖ Done.\")\n",
        "            return result\n",
        "\n",
        "        # Extract F0\n",
        "        # Pass the ABSOLUTE path to the dataset so it works from inside the backend dir\n",
        "        run_cmd(f\"python infer/modules/train/extract/extract_f0_print.py '{dataset_abs_path}' 2 rmvpe\")\n",
        "        \n",
        "        # Extract Features\n",
        "        run_cmd(f\"python infer/modules/train/extract_feature_print.py cuda 1 0 0 '{dataset_abs_path}' v2\")\n",
        "        \n",
        "        # Train\n",
        "        # Using directory names relative to logs/ folder (which is how RVC expects it)\n",
        "        cmd_train = f\"python infer/modules/train/train.py -e {PERSON_NAME} -sr 40k -ov 0 -bs 4 -te {EPOCHS} -pg 0 -if 0 -l 0 -c 0 -sw 0 -v v2\"\n",
        "        run_cmd(cmd_train)\n",
        "        \n",
        "        # 4. Export Model\n",
        "        print(\"‚úÖ Training finished. Exporting model...\")\n",
        "        # Locate generated weights\n",
        "        weights_dir = \"weights\"\n",
        "        # Find latest pth file\n",
        "        pth_files = [f for f in os.listdir(weights_dir) if PERSON_NAME in f and \".pth\" in f]\n",
        "        if pth_files:\n",
        "             latest_model = sorted(pth_files)[-1]\n",
        "             \n",
        "             # Copy to rvcStudioAG models\n",
        "             target_model_path = os.path.join(cwd_backup, \"models\", f\"{PERSON_NAME}.pth\")\n",
        "             shutil.copy(os.path.join(weights_dir, latest_model), target_model_path)\n",
        "             \n",
        "             print(f\"üèÜ Model saved locally to: {target_model_path}\")\n",
        "             \n",
        "             # 5. Backup to Drive\n",
        "             drive_voice_dir = os.path.join(DRIVE_RVC_DIR, PERSON_NAME)\n",
        "             if not os.path.exists(drive_voice_dir):\n",
        "                 os.makedirs(drive_voice_dir)\n",
        "             shutil.copy(target_model_path, os.path.join(drive_voice_dir, f\"{PERSON_NAME}.pth\"))\n",
        "             print(f\"‚òÅÔ∏è Model backed up to Google Drive: {drive_voice_dir}\")\n",
        "        else:\n",
        "             print(\"‚ùå No model file generated.\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Training failed with error: {e}\")\n",
        "    finally:\n",
        "        os.chdir(cwd_backup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé≠ Step 5: Voice Conversion\n",
        "\n",
        "Convert audio using any trained (or loaded) voice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from core.inference import VoiceConverter\n",
        "from utils.registry import discover_voices\n",
        "\n",
        "# List available voices (including those synced from Drive)\n",
        "available_voices = discover_voices(models_dir=\"models\")\n",
        "print(f\"Available voices: {available_voices}\")\n",
        "\n",
        "# Conversion parameters\n",
        "SOURCE_AUDIO = \"/content/source_audio.wav\"  # Path to source audio\n",
        "TARGET_VOICE = available_voices[0] if available_voices else None\n",
        "OUTPUT_PATH = \"/content/output_converted.wav\"\n",
        "\n",
        "if TARGET_VOICE:\n",
        "    print(f\"Converting audio to voice: {TARGET_VOICE}\")\n",
        "    registry = VoiceRegistry(models_dir=\"models\")\n",
        "    model_path = registry.get_model_path(TARGET_VOICE)\n",
        "    \n",
        "    if not model_path:\n",
        "         # Fallback check if registry needs refresh\n",
        "         registry.refresh()\n",
        "         model_path = registry.get_model_path(TARGET_VOICE)\n",
        "\n",
        "    if model_path:\n",
        "        converter = VoiceConverter(model_path, device=device)\n",
        "        try:\n",
        "            converter.convert(SOURCE_AUDIO, OUTPUT_PATH, pitch_shift=0.0)\n",
        "            print(f\"‚úÖ Conversion completed! Output saved to: {OUTPUT_PATH}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Conversion failed: {e}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Could not find model for {TARGET_VOICE}\")\n",
        "else:\n",
        "    print(\"‚ùå No trained voices available.\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}