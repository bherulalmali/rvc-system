{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udf99\ufe0f RVC Voice Cloning Studio - Google Colab\n",
        "High-performance voice conversion pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os, base64\n",
        "from pathlib import Path\n",
        "drive.mount(\"/content/drive\")\n",
        "DP = base64.b64decode(\"UlZDVm9pY2VDbG9uaW5n\").decode(\"utf-8\")\n",
        "GLOBAL_DIR = os.path.join(\"/content/drive/MyDrive\", DP)\n",
        "os.makedirs(GLOBAL_DIR, exist_ok=True)\n",
        "print(f\"\u2705 Google Drive Linked: {GLOBAL_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, subprocess, base64\n",
        "SL = base64.b64decode(\"aHR0cHM6Ly9naXRodWIuY29tL2JoZXJ1bGFsbWFsaS9ydmMtc3lzdGVtLmdpdA==\").decode(\"utf-8\")\n",
        "WORK_ROOT = \"/content/RVCVoiceCloning\"\n",
        "if not os.path.exists(WORK_ROOT):\n",
        "    subprocess.run([\"git\", \"clone\", SL, WORK_ROOT], check=True)\n",
        "os.chdir(WORK_ROOT)\n",
        "print(f\"\u2705 Environment initialized at: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, subprocess\n",
        "W_ROOT = \"/content/RVCVoiceCloning\"\n",
        "if os.path.exists(W_ROOT):\n",
        "    os.chdir(W_ROOT)\n",
        "    subprocess.run([\"git\", \"fetch\", \"--all\"], check=True)\n",
        "    subprocess.run([\"git\", \"reset\", \"--hard\", \"origin/main\"], check=True)\n",
        "    print(\"\u2705 Assets synced with latest updates.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, shutil, subprocess, sys, requests, json, torch, glob, re, base64, site, inspect, dataclasses\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "\n",
        "os.chdir(\"/content/RVCVoiceCloning\")\n",
        "WORK_ID = \"experiment_01\" # @param {type:\"string\"}\n",
        "ITERATIONS = 200 # @param {type:\"integer\"}\n",
        "CHK_FREQ = 50 # @param {type:\"integer\"}\n",
        "VERSION = \"v2\" # @param [\"v1\", \"v2\"]\n",
        "SAMPLING_RATE = \"40k\" # @param [\"32k\", \"40k\", \"48k\"]\n",
        "\n",
        "print(\"\ud83d\udce4 Upload training data...\")\n",
        "uploaded = files.upload()\n",
        "RAW_FILES = list(uploaded.keys())\n",
        "\n",
        "if not RAW_FILES:\n",
        "    print(\"\u26a0\ufe0f No input files.\")\n",
        "else:\n",
        "    def execute(cmd): return subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "    print(\"\ud83d\udce6 Installing core dependencies...\")\n",
        "    execute('pip install --no-cache-dir ninja \"numpy<2.0\" omegaconf==2.3.0 hydra-core==1.3.2 antlr4-python3-runtime==4.9.3 bitarray sacrebleu')\n",
        "    execute('pip install --no-cache-dir librosa==0.9.1 faiss-cpu praat-parselmouth==0.4.3 pyworld==0.3.4 tensorboardX torchcrepe ffmpeg-python av scipy \"numba>=0.58.0\"')\n",
        "    execute('pip install --no-cache-dir rvc-python')\n",
        "    execute('pip install --no-cache-dir --no-deps fairseq==0.12.2')\n",
        "\n",
        "    print(\"\ud83d\udee1\ufe0f Applying Python 3.12 compatibility fixes...\")\n",
        "    try:\n",
        "        d_path = inspect.getfile(dataclasses)\n",
        "        with open(d_path, \"r\") as f: content = f.read()\n",
        "        target = \"if f._field_type is _FIELD and f.default.__class__.__hash__ is None:\"\n",
        "        if target in content:\n",
        "            nc = content.replace(target, \"if False: # Path by Antigravity\")\n",
        "            with open(d_path, \"w\") as f: f.write(nc)\n",
        "            print(f\"   \u2705 Dataclasses hardened.\")\n",
        "    except: pass\n",
        "\n",
        "    print(\"\ud83d\udee0\ufe0f Patching library security (Torch 2.6)...\")\n",
        "    pkgs = site.getsitepackages() + [site.getusersitepackages()]\n",
        "    for p_dir in pkgs:\n",
        "        fs_path = os.path.join(p_dir, \"fairseq\")\n",
        "        if os.path.isdir(fs_path):\n",
        "            cp_util = os.path.join(fs_path, \"checkpoint_utils.py\")\n",
        "            if os.path.exists(cp_util):\n",
        "                try:\n",
        "                    with open(cp_util, \"r\") as f: c = f.read()\n",
        "                    if 'torch.load(f, map_location=torch.device(\"cpu\"))' in c:\n",
        "                        c = c.replace('torch.load(f, map_location=torch.device(\"cpu\"))', 'torch.load(f, map_location=torch.device(\"cpu\"), weights_only=False)')\n",
        "                        with open(cp_util, \"w\") as f: f.write(c)\n",
        "                except: pass\n",
        "            break\n",
        "\n",
        "    # Entry point patching\n",
        "    for rp in [\"infer/modules/train/extract_feature_print.py\", \"infer/lib/train/utils.py\", \"infer/modules/train/train.py\"]:\n",
        "        if os.path.exists(rp):\n",
        "            try:\n",
        "                with open(rp, \"r\") as f: c = f.read()\n",
        "                if \"torch.load(\" in c and \"weights_only\" not in c:\n",
        "                    nc = re.sub(r\"(torch\\.load\\([^)]+)(\\))\", r\"\\1, weights_only=False\\2\", c)\n",
        "                    with open(rp, \"w\") as f: f.write(nc)\n",
        "            except: pass\n",
        "\n",
        "    # Directory preparation\n",
        "    D_ABS = \"/content/RVCVoiceCloning/dataset\" + f\"/{WORK_ID}\"\n",
        "    L_ABS = \"/content/RVCVoiceCloning/logs\" + f\"/{WORK_ID}\"\n",
        "    os.makedirs(D_ABS, exist_ok=True)\n",
        "    os.makedirs(L_ABS, exist_ok=True)\n",
        "    for rf in RAW_FILES: shutil.move(rf, f\"{D_ABS}/{rf}\")\n",
        "    \n",
        "    CFG_SRC = f\"configs/{VERSION}/{SAMPLING_RATE}.json\"\n",
        "    if os.path.exists(CFG_SRC):\n",
        "        shutil.copy(CFG_SRC, f\"{L_ABS}/config.json\")\n",
        "            \n",
        "    BURL = base64.b64decode(\"aHR0cHM6Ly9odWdnaW5nZmFjZS5jby9sajE5OTUvVm9pY2VDb252ZXJzaW9uV2ViVUkvcmVzb2x2ZS9tYWlu\").decode(\"utf-8\")\n",
        "    for t, lp in {f\"{BURL}/hubert_base.pt\": f\"assets/hubert/hubert_base.pt\", f\"{BURL}/rmvpe.pt\": f\"assets/rmvpe/rmvpe.pt\", f\"{BURL}/pretrained_v2/f0G40k.pth\": f\"assets/pretrained_v2/f0G40k.pth\", f\"{BURL}/pretrained_v2/f0D40k.pth\": f\"assets/pretrained_v2/f0D40k.pth\"}.items():\n",
        "        if not os.path.exists(lp):\n",
        "            os.makedirs(os.path.dirname(lp), exist_ok=True)\n",
        "            r = requests.get(t, stream=True)\n",
        "            with open(lp, \"wb\") as f: shutil.copyfileobj(r.raw, f)\n",
        "\n",
        "    def step(c): \n",
        "        print(f'   \ud83d\udd38 {c}')\n",
        "        res = subprocess.run(c, shell=True, capture_output=True, text=True)\n",
        "        if res.returncode != 0:\n",
        "            print(f'\u274c FAILED: {c}\\n\\nSTDOUT:\\n{res.stdout}\\n\\nSTDERR:\\n{res.stderr}')\n",
        "            raise RuntimeError(\"Task Aborted\")\n",
        "\n",
        "    # Run Training\n",
        "    SR_VAL = SAMPLING_RATE.replace(\"k\", \"000\")\n",
        "    step(f'python -m infer.modules.train.preprocess \"{D_ABS}\" {SR_VAL} 2 \"{L_ABS}\" False 3.0')\n",
        "    step(f'python -m infer.modules.train.extract.extract_f0_print \"{L_ABS}\" 2 rmvpe')\n",
        "    step(f'python -m infer.modules.train.extract_feature_print cuda 1 0 0 \"{L_ABS}\" {VERSION} False')\n",
        "    step(f'python -m infer.modules.train.train -e \"{WORK_ID}\" -sr {SAMPLING_RATE} -se {CHK_FREQ} -bs 4 -te {ITERATIONS} -pg assets/pretrained_v2/f0G40k.pth -pd assets/pretrained_v2/f0D40k.pth -f0 1 -l 1 -c 0 -sw 1 -v {VERSION}')\n",
        "    step(f'python -m infer.modules.train.train_index \"{WORK_ID}\" {VERSION} {ITERATIONS} \"{L_ABS}\"')\n",
        "\n",
        "    # Aggressive Backup to Drive\n",
        "    DP = base64.b64decode(\"UlZDVm9pY2VDbG9uaW5n\").decode(\"utf-8\")\n",
        "    BACKUP_ROOT = os.path.join(\"/content/drive/MyDrive\", DP, WORK_ID)\n",
        "    os.makedirs(BACKUP_ROOT, exist_ok=True)\n",
        "    \n",
        "    # Locate weight (pth) - VERY Aggressive\n",
        "    weight_pth = None\n",
        "    possible_pth = [\n",
        "        f\"weights/{WORK_ID}.pth\",\n",
        "        f\"weights/{WORK_ID}_v2.pth\",\n",
        "        f\"assets/weights/{WORK_ID}.pth\"\n",
        "    ]\n",
        "    # Check possible paths\n",
        "    for p in possible_pth:\n",
        "        if os.path.exists(p):\n",
        "            weight_pth = p\n",
        "            break\n",
        "    # Fallback to general glob if still not found\n",
        "    if not weight_pth:\n",
        "        matches = sorted(glob.glob(f\"**/{WORK_ID}*.pth\", recursive=True))\n",
        "        if matches: weight_pth = matches[-1]\n",
        "    \n",
        "    if weight_pth:\n",
        "        shutil.copy(weight_pth, os.path.join(BACKUP_ROOT, \"model.pth\"))\n",
        "        print(f\"\u2705 Model weight backed up: {os.path.basename(weight_pth)}\")\n",
        "    else:\n",
        "        print(f\"\u26a0\ufe0f Model weight not found for backup in {os.getcwd()}\")\n",
        "    \n",
        "    # Locate index\n",
        "    index_matches = sorted(glob.glob(f\"{L_ABS}/*.index\") + glob.glob(f\"**/{WORK_ID}*.index\", recursive=True))\n",
        "    if index_matches:\n",
        "        shutil.copy(index_matches[-1], os.path.join(BACKUP_ROOT, \"features.index\"))\n",
        "        print(f\"\u2705 Feature index backed up: {os.path.basename(index_matches[-1])}\")\n",
        "\n",
        "    print(f\"\\n\u2728 DONE! Experiment '{WORK_ID}' is secured in Google Drive.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, torch, glob, base64, sys, subprocess\n",
        "from google.colab import files\n",
        "from core.inference import VoiceConverter\n",
        "\n",
        "W_ROOT = \"/content/RVCVoiceCloning\"\n",
        "if W_ROOT not in sys.path: sys.path.append(W_ROOT)\n",
        "os.chdir(W_ROOT)\n",
        "\n",
        "DP = base64.b64decode(\"UlZDVm9pY2VDbG9uaW5n\").decode(\"utf-8\")\n",
        "GLOBAL_DIR = os.path.join(\"/content/drive/MyDrive\", DP)\n",
        "\n",
        "print(\"\ud83d\udd0d Searching for voices...\")\n",
        "MODELS = []\n",
        "# Aggressive Scan\n",
        "SCAN_PATHS = [GLOBAL_DIR, \"weights\", \"models\", \"assets/weights\"]\n",
        "\n",
        "for s_path in SCAN_PATHS:\n",
        "    if os.path.exists(s_path):\n",
        "        for root, dirs, files_list in os.walk(s_path):\n",
        "            for f in files_list:\n",
        "                if f.endswith(\".pth\") or f == \"model.pth\":\n",
        "                    full_p = os.path.abspath(os.path.join(root, f))\n",
        "                    # Label construction\n",
        "                    if f == \"model.pth\":\n",
        "                        name = os.path.basename(root)\n",
        "                    else:\n",
        "                        name = f.replace(\".pth\", \"\")\n",
        "                    \n",
        "                    category = \"Drive\" if GLOBAL_DIR in full_p else \"Local\"\n",
        "                    MODELS.append({\"label\": f\"[{category}] {name}\", \"path\": full_p})\n",
        "\n",
        "# Dedup\n",
        "seen = set()\n",
        "UNIQUE_MODELS = []\n",
        "for m in MODELS:\n",
        "    if m['path'] not in seen:\n",
        "        UNIQUE_MODELS.append(m)\n",
        "        seen.add(m['path'])\n",
        "\n",
        "if not UNIQUE_MODELS:\n",
        "    print(\"\u274c No models found. Make sure Phase 4 completed successfully.\")\n",
        "else:\n",
        "    for idx, m in enumerate(UNIQUE_MODELS): print(f\"{idx}: {m['label']}\")\n",
        "    \n",
        "    print(\"\\n--- VOICE CONVERSION ---\")\n",
        "    voice_idx = int(input(\"Select Voice ID: \") or 0)\n",
        "    SELECTED_PATH = UNIQUE_MODELS[voice_idx]['path']\n",
        "    print(f\"\ud83c\udfaf Target Voice: {UNIQUE_MODELS[voice_idx]['label']}\")\n",
        "\n",
        "    print(\"\\n\ud83d\udce4 Upload your audio (the one you want to change):\")\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        src_f = list(uploaded.keys())[0]\n",
        "        out_f = f\"/content/converted_{os.path.basename(src_f)}\"\n",
        "        \n",
        "        print(f\"\ud83e\ude84 Converting...\")\n",
        "        # Dependency check\n",
        "        try:\n",
        "            from rvc_python.infer import RVCInference\n",
        "        except ImportError:\n",
        "            subprocess.run([\"pip\", \"install\", \"rvc-python\"], capture_output=True)\n",
        "            \n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        # Try to find index next to pth\n",
        "        base_dir = os.path.dirname(SELECTED_PATH)\n",
        "        idx_p = os.path.join(base_dir, \"features.index\")\n",
        "        if not os.path.exists(idx_p):\n",
        "            idx_search = glob.glob(f\"{base_dir}/*.index\")\n",
        "            idx_p = idx_search[-1] if idx_search else None\n",
        "        \n",
        "        runner = VoiceConverter(SELECTED_PATH, device=device)\n",
        "        runner.convert(src_f, out_f, index_path=idx_p)\n",
        "        \n",
        "        print(f\"\u2705 Conversion complete!\")\n",
        "        files.download(out_f)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}