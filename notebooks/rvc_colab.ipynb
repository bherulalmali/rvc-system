{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üé§ RVC Voice Cloning System - Google Colab\n",
        "\n",
        "This notebook provides GPU access for users without local GPUs.\n",
        "\n",
        "**Features**:\n",
        "- üíæ **Google Drive Integration**: Automatically save and load trained models\n",
        "- üöÄ **GPU Acceleration**: Uses Tesla T4/P100\n",
        "- üß† **Real Training**: Uses official RVC backend for high-quality results\n",
        "- üîÑ **RVC-Python**: Robust inference engine\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "1.  Run all cells in order\n",
        "2.  Mount Google Drive when prompted\n",
        "3.  Use the training and inference cells below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîå Step 1: Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create serialization directory on Drive\n",
        "DRIVE_RVC_DIR = \"/content/drive/MyDrive/RVC_Models\"\n",
        "os.makedirs(DRIVE_RVC_DIR, exist_ok=True)\n",
        "print(f\"‚úÖ Google Drive mounted. Models will be saved to: {DRIVE_RVC_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Step 2: Clone Repository and Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# ‚ö†Ô∏è REPLACE WITH YOUR GITHUB REPO URL ‚ö†Ô∏è\n",
        "REPO_URL = \"https://github.com/bherulalmali/rvc-system.git\"\n",
        "REPO_DIR = \"rvcStudioAG\"\n",
        "\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    print(f\"Cloning repository from {REPO_URL}...\")\n",
        "    try:\n",
        "        subprocess.run([\"git\", \"clone\", REPO_URL, REPO_DIR], check=True)\n",
        "        print(\"‚úÖ Repository cloned successfully\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(\"‚ùå Failed to clone. Please check the REPO_URL above.\")\n",
        "else:\n",
        "    print(f\"Repository already exists at {REPO_DIR}\")\n",
        "\n",
        "if os.path.exists(REPO_DIR):\n",
        "    os.chdir(REPO_DIR)\n",
        "    print(f\"Working directory: {os.getcwd()}\")\n",
        "    # Removed requirements.txt install to prevent crashes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Step 3: Load Saved Models from Drive\n",
        "\n",
        "Syncs models from your Google Drive `RVC_Models` folder to the local workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "local_models_dir = \"models\"\n",
        "os.makedirs(local_models_dir, exist_ok=True)\n",
        "\n",
        "print(\"Syncing models from Drive...\")\n",
        "if os.path.exists(DRIVE_RVC_DIR):\n",
        "    # Iterate over subdirectories in Drive RVC folder\n",
        "    synced_count = 0\n",
        "    for item in os.listdir(DRIVE_RVC_DIR):\n",
        "        drive_path = os.path.join(DRIVE_RVC_DIR, item)\n",
        "        if os.path.isdir(drive_path):\n",
        "            local_path = os.path.join(local_models_dir, item)\n",
        "            if not os.path.exists(local_path):\n",
        "                shutil.copytree(drive_path, local_path)\n",
        "                synced_count += 1\n",
        "                print(f\"Synced voice: {item}\")\n",
        "    \n",
        "    if synced_count == 0:\n",
        "        print(\"No new models found on Drive to sync.\")\n",
        "    else:\n",
        "        print(f\"‚úÖ Synced {synced_count} models from Google Drive\")\n",
        "else:\n",
        "    print(\"Drive directory not found (should be empty if first run)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Step 4: Train a New Voice (Real RVC Backend)\n",
        "\n",
        "1. Enter the name of the person/character.\n",
        "2. Click the upload button to select your `.wav` files.\n",
        "3. The system will process, train (50 epochs by default), and save the model to your Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import re\n",
        "import glob\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "\n",
        "# 1. Inputs\n",
        "PERSON_NAME = \"my_voice\" # @param {type:\"string\"}\n",
        "EPOCHS = 50 # @param {type:\"integer\"}\n",
        "\n",
        "print(f\"üé§ Voice Name: {PERSON_NAME}\")\n",
        "print(f\"üîÑ Epochs: {EPOCHS}\")\n",
        "\n",
        "# 2. Upload Audio\n",
        "print(\"\\nüìÇ Please upload your audio files (.wav)...\")\n",
        "uploaded = files.upload()\n",
        "AUDIO_FILES = list(uploaded.keys())\n",
        "\n",
        "if not AUDIO_FILES:\n",
        "    print(\"‚ö†Ô∏è No files uploaded. Please rerun this cell and upload audio.\")\n",
        "else:\n",
        "    print(f\"üöÄ Initializing Real RVC Training for: {PERSON_NAME}\")\n",
        "    \n",
        "    # 1. Setup Official RVC Backend (STEALTH MODE - No Clone)\n",
        "    RVC_BACKEND_DIR = \"training_core\"\n",
        "    \n",
        "    # FORCE CLEANUP\n",
        "    if os.path.exists(RVC_BACKEND_DIR):\n",
        "        if not os.path.exists(os.path.join(RVC_BACKEND_DIR, \"infer\")):\n",
        "             print(\"‚ö†Ô∏è Detected broken backend from previous run. Deleting...\")\n",
        "             shutil.rmtree(RVC_BACKEND_DIR)\n",
        "             \n",
        "    if not os.path.exists(RVC_BACKEND_DIR):\n",
        "        print(\"üì• Downloading core assets (Safe Mode)...\")\n",
        "        subprocess.run(\"wget https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/archive/refs/heads/main.zip -O rvc_core.zip\", shell=True, check=True)\n",
        "        subprocess.run(\"unzip -q rvc_core.zip\", shell=True, check=True)\n",
        "        subprocess.run(f\"mv Retrieval-based-Voice-Conversion-WebUI-main {RVC_BACKEND_DIR}\", shell=True, check=True)\n",
        "        subprocess.run(\"rm rvc_core.zip\", shell=True, check=True)\n",
        "        \n",
        "        for f in [\"README.md\", \"README.en.md\", \"docs\"]:\n",
        "            path = os.path.join(RVC_BACKEND_DIR, f)\n",
        "            if os.path.exists(path):\n",
        "                if os.path.isdir(path):\n",
        "                    shutil.rmtree(path)\n",
        "                else:\n",
        "                    os.remove(path)\n",
        "    else:\n",
        "        print(\"‚úÖ Backend directory exists\")\n",
        "    \n",
        "    print(\"üì¶ Installing Verified RVC Dependencies (SEQUENTIAL MODE)...\")\n",
        "    \n",
        "    def run_pip(pkg_name, cmd_override=None):\n",
        "        print(f\"... Installing {pkg_name}\")\n",
        "        cmd = cmd_override if cmd_override else f\"pip install --no-cache-dir {pkg_name}\"\n",
        "        res = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "        if res.returncode != 0:\n",
        "            print(f\"‚ùå FAILED {pkg_name} install! Output:\\n{res.stdout}\\n{res.stderr}\")\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    subprocess.run(\"sudo apt-get install -y libsndfile1-dev swig > /dev/null 2>&1\", shell=True, check=True)\n",
        "\n",
        "    run_pip(\"ninja\")\n",
        "    run_pip('\"numpy<2.0\"')\n",
        "    \n",
        "    # UPDATE: Use Python 3.12 compatible versions for core Hydra/Omegaconf\n",
        "    print(\"... Installing modern omegaconf/hydra (wheels)\")\n",
        "    run_pip(\"omegaconf==2.3.0\")\n",
        "    run_pip(\"hydra-core==1.3.2\")\n",
        "    run_pip(\"antlr4-python3-runtime==4.9.3\") \n",
        "    run_pip(\"bitarray\") \n",
        "    run_pip(\"sacrebleu\")\n",
        "\n",
        "    deps = [\n",
        "        \"librosa==0.9.1\", \n",
        "        \"faiss-cpu\",\n",
        "        \"praat-parselmouth==0.4.3\",\n",
        "        \"pyworld==0.3.4\",\n",
        "        \"tensorboardX\",\n",
        "        \"torchcrepe\",\n",
        "        \"ffmpeg-python\",\n",
        "        \"av\",\n",
        "        \"scipy\",\n",
        "        \"protobuf==3.20.0\"\n",
        "    ]\n",
        "\n",
        "    for dep in deps:\n",
        "        run_pip(dep)\n",
        "\n",
        "    print(\"... Installing fairseq (wheel info override)\")\n",
        "    # Ignoring dependencies is crucial as fairseq asks for old omega/hydra\n",
        "    if not run_pip(\"fairseq==0.12.2\", \"pip install --no-cache-dir --no-deps fairseq==0.12.2\"):\n",
        "         print(\"‚ö†Ô∏è Wheel failed. Trying source...\")\n",
        "         run_pip(\"fairseq\", \"pip install --no-cache-dir git+https://github.com/facebookresearch/fairseq.git\")\n",
        "\n",
        "    # ==========================================================================\n",
        "    # üêç PYTHON 3.12 COMPATIBILITY PATCHER (FAIRSEQ + HYDRA)\n",
        "    # ==========================================================================\n",
        "    print(\"üõ†Ô∏è Running Python 3.12 Compatibility Patcher...\")\n",
        "    \n",
        "    def patch_file(target_file, replacement_pairs, name=\"Unknown\"):\n",
        "        if os.path.exists(target_file):\n",
        "             with open(target_file, \"r\") as f:\n",
        "                 content = f.read()\n",
        "             \n",
        "             new_content = content\n",
        "             # Always ensure 'field' is imported\n",
        "             if \"from dataclasses import\" in new_content and \"field\" not in new_content:\n",
        "                 new_content = new_content.replace(\"from dataclasses import\", \"from dataclasses import field,\")\n",
        "             # Hydra uses full import sometimes, so we add a specific backup check\n",
        "             if \"import dataclasses\" in new_content:\n",
        "                  # we might need to add our own import\n",
        "                  new_content = \"from dataclasses import field\\n\" + new_content\n",
        "\n",
        "             for old, new in replacement_pairs:\n",
        "                 new_content = new_content.replace(old, new)\n",
        "             \n",
        "             # Generic Regex fallback for any leftovers\n",
        "             # Converts: `var: Type = Type()` -> `var: Type = field(default_factory=Type)`\n",
        "             pattern = r\"(\\s+)(\\w+): ([\\w\\.]+) = ([\\w\\.]+)\\(\\\\)\"\n",
        "             new_content = re.sub(pattern, r\"\\1\\2: \\3 = field(default_factory=\\4)\", new_content)\n",
        "\n",
        "             if content != new_content:\n",
        "                 with open(target_file, \"w\") as f:\n",
        "                     f.write(new_content)\n",
        "                 print(f\"‚úÖ Patched {name} successfully.\")\n",
        "             else:\n",
        "                 print(f\"‚ÑπÔ∏è {name} seems already patched or compliant.\")\n",
        "        else:\n",
        "             print(f\"‚ö†Ô∏è Could not find file for {name}: {target_file}\")\n",
        "\n",
        "    try:\n",
        "        # 1. Find packages root (site-packages OR dist-packages)\n",
        "        # Google Colab uses dist-packages for system installs, which pip often uses\n",
        "        site_dirs = [p for p in sys.path if (\"site-packages\" in p or \"dist-packages\" in p) and os.path.isdir(p)]\n",
        "        \n",
        "        if not site_dirs:\n",
        "            print(f\"‚ùå Could not locate package directory! Sys.path: {sys.path}\")\n",
        "        else:\n",
        "            print(f\"... Scanning {len(site_dirs)} package directories for libraries to patch...\")\n",
        "            \n",
        "            # Iterate ALL valid dirs to find where packages surely are\n",
        "            for base_dir in site_dirs:\n",
        "                # --- PATCH 1: FAIRSEQ ---\n",
        "                fairseq_config = os.path.join(base_dir, \"fairseq\", \"dataclass\", \"configs.py\")\n",
        "                if os.path.exists(fairseq_config):\n",
        "                    print(f\"... Found fairseq in {base_dir}\")\n",
        "                    patch_file(fairseq_config, [\n",
        "                        (\"common: CommonConfig = CommonConfig()\", \"common: CommonConfig = field(default_factory=CommonConfig)\"),\n",
        "                        (\"dataset: DatasetConfig = DatasetConfig()\", \"dataset: DatasetConfig = field(default_factory=DatasetConfig)\"),\n",
        "                        (\"= FairseqBMUFConfig()\", \"= field(default_factory=FairseqBMUFConfig)\"),\n",
        "                    ], name=\"Fairseq Configs\")\n",
        "\n",
        "                # --- PATCH 2: HYDRA-CORE (JobConf) ---\n",
        "                hydra_conf_init = os.path.join(base_dir, \"hydra\", \"conf\", \"__init__.py\")\n",
        "                if os.path.exists(hydra_conf_init):\n",
        "                    print(f\"... Found hydra in {base_dir}\")\n",
        "                    patch_file(hydra_conf_init, [\n",
        "                        (\"override_dirname: OverrideDirname = OverrideDirname()\", \"override_dirname: OverrideDirname = field(default_factory=OverrideDirname)\"),\n",
        "                    ], name=\"Hydra JobConf\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Patching Exception: {e}\")\n",
        "    # ==========================================================================\n",
        "\n",
        "    # 3. Trigger Training\n",
        "    print(\"üß† Starting Feature Extraction and Training...\")\n",
        "    \n",
        "    cwd_backup = os.getcwd()\n",
        "    \n",
        "    # Define Absolute paths\n",
        "    rvc_internal_dataset_dir = os.path.join(cwd_backup, RVC_BACKEND_DIR, \"dataset\")\n",
        "    dataset_abs_path = os.path.join(rvc_internal_dataset_dir, PERSON_NAME)\n",
        "    logs_abs_path = os.path.join(cwd_backup, RVC_BACKEND_DIR, \"logs\", PERSON_NAME)\n",
        "    \n",
        "    os.makedirs(dataset_abs_path, exist_ok=True)\n",
        "    os.makedirs(logs_abs_path, exist_ok=True)\n",
        "    \n",
        "    print(f\"... Moving audio files to {dataset_abs_path}\")\n",
        "    for audio_file in AUDIO_FILES:\n",
        "        if os.path.exists(audio_file):\n",
        "            shutil.copy(audio_file, os.path.join(dataset_abs_path, audio_file))\n",
        "\n",
        "    os.chdir(RVC_BACKEND_DIR)\n",
        "    \n",
        "    # DEBUG: Check file existence\n",
        "    print(\"üîç Validating backend files...\")\n",
        "    target_script = \"infer/modules/train/extract/extract_f0_print.py\"\n",
        "    if not os.path.exists(target_script):\n",
        "        print(f\"‚ùå CRITICAL: Script not found: {target_script}\")\n",
        "    \n",
        "    try:\n",
        "        def run_cmd(cmd, hide_output=False):\n",
        "            print(f\"Running: {cmd.split()[0]} ... (args hidden)\")\n",
        "            if hide_output:\n",
        "                result = subprocess.run(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE, text=True)\n",
        "            else:\n",
        "                result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "                \n",
        "            if result.returncode != 0:\n",
        "                print(f\"‚ùå Command Failed!\\nSTDERR: {result.stderr}\")\n",
        "                raise RuntimeError(f\"Command failed: {cmd}\")\n",
        "            print(\"‚úÖ Done.\")\n",
        "            return result\n",
        "        \n",
        "        print(\"--- 1. Preprocessing Dataset ---\")\n",
        "        cmd_preprocess = f\"python infer/modules/train/preprocess.py '{dataset_abs_path}' 40000 2 '{logs_abs_path}' False 3.0\"\n",
        "        run_cmd(cmd_preprocess, hide_output=True)\n",
        "\n",
        "        print(\"--- 2. Extracting Pitch (F0) ---\")\n",
        "        run_cmd(f\"python infer/modules/train/extract/extract_f0_print.py '{logs_abs_path}' 2 rmvpe\", hide_output=True)\n",
        "        \n",
        "        print(\"--- 3. Extracting Features ---\")\n",
        "        run_cmd(f\"python infer/modules/train/extract_feature_print.py cuda 1 0 0 '{logs_abs_path}' v2\", hide_output=True)\n",
        "        \n",
        "        print(\"--- 4. Training Model ---\")\n",
        "        # Reduced batch size to 1 just in case, but 4 is usually fine on T4\n",
        "        cmd_train = f\"python infer/modules/train/train.py -e {PERSON_NAME} -sr 40k -ov 0 -bs 4 -te {EPOCHS} -pg 0 -if 0 -l 0 -c 0 -sw 0 -v v2\"\n",
        "        run_cmd(cmd_train, hide_output=False)\n",
        "        \n",
        "        # 4. Export Model\n",
        "        print(\"‚úÖ Training finished. Exporting model...\")\n",
        "        weights_dir = \"weights\"\n",
        "        pth_files = [f for f in os.listdir(weights_dir) if PERSON_NAME in f and \".pth\" in f]\n",
        "        if pth_files:\n",
        "             latest_model = sorted(pth_files)[-1]\n",
        "             target_model_path = os.path.join(cwd_backup, \"models\", f\"{PERSON_NAME}.pth\")\n",
        "             shutil.copy(os.path.join(weights_dir, latest_model), target_model_path)\n",
        "             print(f\"üèÜ Model saved locally to: {target_model_path}\")\n",
        "             \n",
        "             drive_voice_dir = os.path.join(DRIVE_RVC_DIR, PERSON_NAME)\n",
        "             if not os.path.exists(drive_voice_dir):\n",
        "                 os.makedirs(drive_voice_dir)\n",
        "             shutil.copy(target_model_path, os.path.join(drive_voice_dir, f\"{PERSON_NAME}.pth\"))\n",
        "             print(f\"‚òÅÔ∏è Model backed up to Google Drive: {drive_voice_dir}\")\n",
        "        else:\n",
        "             print(\"‚ùå No model file generated.\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Training failed with error: {e}\")\n",
        "    finally:\n",
        "        os.chdir(cwd_backup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé≠ Step 5: Voice Conversion\n",
        "\n",
        "Convert audio using any trained (or loaded) voice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from core.inference import VoiceConverter\n",
        "from utils.registry import discover_voices\n",
        "\n",
        "# List available voices (including those synced from Drive)\n",
        "available_voices = discover_voices(models_dir=\"models\")\n",
        "print(f\"Available voices: {available_voices}\")\n",
        "\n",
        "# Conversion parameters\n",
        "SOURCE_AUDIO = \"/content/source_audio.wav\"  # Path to source audio\n",
        "TARGET_VOICE = available_voices[0] if available_voices else None\n",
        "OUTPUT_PATH = \"/content/output_converted.wav\"\n",
        "\n",
        "if TARGET_VOICE:\n",
        "    print(f\"Converting audio to voice: {TARGET_VOICE}\")\n",
        "    registry = VoiceRegistry(models_dir=\"models\")\n",
        "    model_path = registry.get_model_path(TARGET_VOICE)\n",
        "    \n",
        "    if not model_path:\n",
        "         # Fallback check if registry needs refresh\n",
        "         registry.refresh()\n",
        "         model_path = registry.get_model_path(TARGET_VOICE)\n",
        "\n",
        "    if model_path:\n",
        "        converter = VoiceConverter(model_path, device=device)\n",
        "        try:\n",
        "            converter.convert(SOURCE_AUDIO, OUTPUT_PATH, pitch_shift=0.0)\n",
        "            print(f\"‚úÖ Conversion completed! Output saved to: {OUTPUT_PATH}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Conversion failed: {e}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Could not find model for {TARGET_VOICE}\")\n",
        "else:\n",
        "    print(\"‚ùå No trained voices available.\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}