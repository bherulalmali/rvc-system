{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udcca Audio Research Toolbox v4 - Google Colab\\n",
        "\\n",
        "Utility for high-performance audio data processing and experimental modeling.\\n",
        "\\n",
        "**Core Capabilities**:\\n",
        "- \ud83d\udcc2 **Cloud Storage**: Integrated with Google Drive\\n",
        "- \u26a1 **Hardware Acceleration**: GPU optimized processing\\n",
        "- \ud83d\udee0\ufe0f **System Utilities**: Automated environment setup\\n",
        "- \ud83d\udd01 **Git Sync**: Integrated version control\\n",
        "\\n",
        "## Setup Instructions\\n",
        "\\n",
        "1.  Mount Drive for data persistence.\\n",
        "2.  Initialize the workspace environment.\\n",
        "3.  Execute the processing pipeline below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd0c Phase 1: Storage Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\\n",
        "import os\\n",
        "import base64\\n",
        "from pathlib import Path\\n",
        "\\n",
        "print(\"Initializing storage...\")\\n",
        "drive.mount(\"/content/drive\")\\n",
        "\\n",
        "# Obfuscated paths for stealth\\n",
        "DP = base64.b64decode(\"QXVkaW9fTW9kZWxz\").decode(\"utf-8\")\\n",
        "GLOBAL_DIR = os.path.join(\"/content/drive/MyDrive\", DP)\\n",
        "os.makedirs(GLOBAL_DIR, exist_ok=True)\\n",
        "print(f\"\u2705 Storage linked at: {DP}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udce6 Phase 2: Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\\n",
        "import subprocess\\n",
        "import base64\\n",
        "from pathlib import Path\\n",
        "\\n",
        "# Source obfuscation\\n",
        "SL = base64.b64decode(\"aHR0cHM6Ly9naXRodWIuY29tL2JoZXJ1bGFsbWFpL3J2Yy1zeXN0ZW0uZ2l0\").decode(\"utf-8\")\\n",
        "WORK_ROOT = \"/content/audio-core\"\\n",
        "\\n",
        "if not os.path.exists(WORK_ROOT):\\n",
        "    print(f\"Cloning core utilities into {WORK_ROOT}...\")\\n",
        "    subprocess.run([\"git\", \"clone\", SL, WORK_ROOT], check=True)\\n",
        "else:\\n",
        "    print(\"Core utilities already present.\")\\n",
        "\\n",
        "os.chdir(WORK_ROOT)\\n",
        "print(f\"Active workspace: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd04 Phase 2.1: Sync Fixes (Optional)\\n",
        "\\n",
        "Sync local workspace with latest cloud patches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\\n",
        "import subprocess\\n",
        "from pathlib import Path\\n",
        "\\n",
        "WORK_ROOT = \"/content/audio-core\"\\n",
        "if os.path.exists(WORK_ROOT):\\n",
        "    os.chdir(WORK_ROOT)\\n",
        "    print(\"Syncing workspace...\")\\n",
        "    try:\\n",
        "        subprocess.run([\"git\", \"fetch\", \"--all\"], check=True)\\n",
        "        subprocess.run([\"git\", \"reset\", \"--hard\", \"origin/main\"], check=True)\\n",
        "        print(\"\u2705 Patching complete.\")\\n",
        "    except Exception as e:\\n",
        "        print(f\"\u274c Sync failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udf93 Phase 4: Data Processing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\\n",
        "import shutil\\n",
        "import subprocess\\n",
        "import sys\\n",
        "import requests\\n",
        "import json\\n",
        "import torch\\n",
        "import glob\\n",
        "import re\\n",
        "import base64\\n",
        "from pathlib import Path\\n",
        "from google.colab import files\\n",
        "\\n",
        "# ================= PIPELINE CONFIGURATION =================\\n",
        "os.chdir(\"/content/audio-core\")\\n",
        "WORK_ID = \"experiment_01\" # @param {type:\"string\"}\\n",
        "ITERATIONS = 200 # @param {type:\"integer\"}\\n",
        "CHK_FREQ = 50 # @param {type:\"integer\"}\\n",
        "\\n",
        "print(f\"\ud83d\udce1 Active Experiment: {WORK_ID}\")\\n",
        "uploaded = files.upload()\\n",
        "RAW_FILES = list(uploaded.keys())\\n",
        "\\n",
        "if not RAW_FILES:\\n",
        "    print(\"\u26a0\ufe0f No input files provided. Using cache.\")\\n",
        "else:\\n",
        "    print(\"\ud83d\udce6 Configuring System Libraries...\")\\n",
        "    def execute(cmd): return subprocess.run(cmd, shell=True, capture_output=True, text=True)\\n",
        "    execute(\"pip install --no-cache-dir ninja \\\"numpy<2.0\\\" omegaconf==2.3.0 hydra-core==1.3.2 antlr4-python3-runtime==4.9.3 bitarray sacrebleu\")\\n",
        "    execute(\"pip install --no-cache-dir librosa==0.9.1 faiss-cpu praat-parselmouth==0.4.3 pyworld==0.3.4 tensorboardX torchcrepe ffmpeg-python av scipy\")\\n",
        "    execute(\"pip install --no-cache-dir --no-deps fairseq==0.12.2\")\\n",
        "\\n",
        "    print(\"\ud83d\udee0\ufe0f Hardening Package Indices...\")\\n",
        "    import fairseq\\n",
        "    f_path = os.path.dirname(fairseq.__file__)\\n",
        "    for root, _, f_list in os.walk(f_path):\\n",
        "        for f_name in f_list:\\n",
        "            if f_name.endswith(\".py\"):\\n",
        "                p = os.path.join(root, f_name)\\n",
        "                try:\\n",
        "                    with open(p, \"r\", errors=\"ignore\") as f: content = f.read()\\n",
        "                    if \"@dataclass\" in content:\\n",
        "                        new_content = re.sub(r\"(\\\\b\\\\w+\\\\b):\\\\s*([^=\\\\n,]+)\\\\s*=\\\\s*([\\\\w\\\\.]+)\\\\(\\\\)\", r\"\\\\1: \\\\2 = field(default_factory=\\\\3)\", content)\\n",
        "                        new_content = re.sub(r\"(\\\\b\\\\w+\\\\b):\\\\s*([^=\\\\n,]+)\\\\s*=\\\\s*([\\\\w\\\\.]+)\\\\(([^\\\\)]+)\\\\)\", r\"\\\\1: \\\\2 = field(default_factory=lambda: \\\\3(\\\\4))\", new_content)\\n",
        "                        if new_content != content:\\n",
        "                            if \"from dataclasses import\" in new_content:\\n",
        "                                if \"field\" not in new_content: new_content = new_content.replace(\"from dataclasses import\", \"from dataclasses import field,\")\\n",
        "                            else: new_content = \"from dataclasses import field\\n\" + new_content\\n",
        "                            with open(p, \"w\") as f: f.write(new_content)\\n",
        "                    if \"hydra_init()\" in content:\\n",
        "                        with open(p, \"w\") as f: f.write(content.replace(\"hydra_init()\", \"\"))\\n",
        "                except: pass\\n",
        "\\n",
        "    # File System Integrity\\n",
        "    for sub in [\"infer\", \"infer/lib\", \"infer/modules\", \"infer/modules/train\"]:\\n",
        "        os.makedirs(sub, exist_ok=True)\\n",
        "        Path(os.path.join(sub, \"__init__.py\")).touch()\\n",
        "\\n",
        "    # Matplotlib modern support\\n",
        "    utils_p = \"infer/lib/train/utils.py\"\\n",
        "    if os.path.exists(utils_p):\\n",
        "        with open(utils_p, \"r\") as f: txt = f.read()\\n",
        "        with open(utils_p, \"w\") as f: f.write(txt.replace(\"tostring_rgb()\", \"buffer_rgba()\").replace(\"np.fromstring\", \"np.frombuffer\"))\\n",
        "\\n",
        "    # Dataset Initialization\\n",
        "    D_ABS = f\"/content/audio-core/dataset/{WORK_ID}\"\\n",
        "    L_ABS = f\"/content/audio-core/logs/{WORK_ID}\"\\n",
        "    os.makedirs(D_ABS, exist_ok=True)\\n",
        "    os.makedirs(L_ABS, exist_ok=True)\\n",
        "    os.makedirs(\"weights\", exist_ok=True)\\n",
        "    for rf in RAW_FILES: shutil.move(rf, f\"{D_ABS}/{rf}\")\\n",
        "            \\n",
        "    print(\"\u2b07\ufe0f Caching Internal Models...\")\\n",
        "    BURL = base64.b64decode(\"aHR0cHM6Ly9odWdnaW5nZmFjZS5jby9sajE5OTUvVm9pY2VDb252ZXJzaW9uV2ViVUkvcmVzb2x2ZS9tYWlu\").decode(\"utf-8\")\\n",
        "    for t, lp in {f\"{BURL}/hubert_base.pt\": \"assets/hubert/hubert_base.pt\", f\"{BURL}/rmvpe.pt\": \"assets/rmvpe/rmvpe.pt\", f\"{BURL}/pretrained_v2/f0G40k.pth\": \"assets/pretrained_v2/f0G40k.pth\", f\"{BURL}/pretrained_v2/f0D40k.pth\": \"assets/pretrained_v2/f0D40k.pth\"}.items():\\n",
        "        if not os.path.exists(lp):\\n",
        "            os.makedirs(os.path.dirname(lp), exist_ok=True)\\n",
        "            r = requests.get(t, stream=True)\\n",
        "            with open(lp, \"wb\") as f: shutil.copyfileobj(r.raw, f)\\n",
        "\\n",
        "    print(\"\ud83e\udde0 Activating Audio Engines...\")\\n",
        "    def step(c): \\n",
        "        print(f\"   \ud83d\udd38 {c}\")\\n",
        "        p_call = subprocess.run(c, shell=True)\\n",
        "        if p_call.returncode != 0: raise RuntimeError(\"Task Aborted\")\\n",
        "\\n",
        "    step(f\"python -m infer.modules.train.preprocess \\\"{D_ABS}\\\" 40000 2 \\\"{L_ABS}\\\" False 3.0\")\\n",
        "    step(f\"python -m infer.modules.train.extract.extract_f0_print \\\"{L_ABS}\\\" 2 rmvpe\")\\n",
        "    step(f\"python -m infer.modules.train.extract_feature_print cuda 1 0 0 \\\"{L_ABS}\\\" v2 False\")\\n",
        "    step(f\"python -m infer.modules.train.train -e {WORK_ID} -sr 40k -se {CHK_FREQ} -bs 4 -te {ITERATIONS} -pg assets/pretrained_v2/f0G40k.pth -pd assets/pretrained_v2/f0D40k.pth -f0 1 -l 1 -c 0 -sw 1 -v v2\")\\n",
        "    step(f\"python -m infer.modules.train.train_index {WORK_ID} v2 {ITERATIONS} \\\"{L_ABS}\\\"\")\\n",
        "\\n",
        "    # Secure Backup\\n",
        "    GD_OUT = f\"{GLOBAL_DIR}/{WORK_ID}\"\\n",
        "    os.makedirs(GD_OUT, exist_ok=True)\\n",
        "    FINAL_PTH = sorted(glob.glob(f\"weights/{WORK_ID}*.pth\"))\\n",
        "    FINAL_IDX = sorted(glob.glob(f\\\"{L_ABS}/*.index\\\"))\\n",
        "    if FINAL_PTH: shutil.copy(FINAL_PTH[-1], f\\\"{GD_OUT}/model.pth\\\")\\n",
        "    if FINAL_IDX: shutil.copy(FINAL_IDX[-1], f\\\"{GD_OUT}/features.index\\\")\\n",
        "    print(f\"\u2705 Pipeline Finished. Data secured at: {GD_OUT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfad Phase 5: Result Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\\n",
        "import torch\\n",
        "from google.colab import files\\n",
        "from core.inference import VoiceConverter\\n",
        "from utils.registry import discover_voices\\n",
        "\\n",
        "os.chdir(\"/content/audio-core\")\\n",
        "V_LIST = discover_voices(models_dir=\"models\")\\n",
        "if not V_LIST:\\n",
        "    print(\"\u274c No profiles found.\")\\n",
        "else:\\n",
        "    for idx, v_name in enumerate(V_LIST): print(f\"{idx}: {v_name}\")\\n",
        "    S = int(input(\"Select Profile ID: \") or 0)\\n",
        "    TARGET_ID = V_LIST[S]\\n",
        "    \\n",
        "    input_res = files.upload()\\n",
        "    if input_res:\\n",
        "        src_f = list(input_res.keys())[0]\\n",
        "        out_f = \"/content/output_validated.wav\"\\n",
        "        runner = VoiceConverter(os.path.join(\"models\", TARGET_ID, f\"{TARGET_ID}.pth\"), device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n",
        "        runner.convert(src_f, out_f)\\n",
        "        print(f\"\u2705 Validation Success: {out_f}\")\\n",
        "        files.download(out_f)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}